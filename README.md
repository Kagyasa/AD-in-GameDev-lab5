# АНАЛИЗ ДАННЫХ И ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ [in GameDev]
Отчет по лабораторной работе #5 выполнил(а):
- Булатова Алина Дамировна
- РИ230915
Отметка о выполнении заданий (заполняется студентом):

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 20 |
| Задание 3 | * | 20 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

Структура отчета

- Данные о работе: название работы, фио, группа, выполненные задания.
- Цель работы.
- Задание 1.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 2.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 3.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Выводы.
- ✨Magic ✨

## Цель работы
познакомиться с программными средствами для создания системы машинного обучения и ее интеграции в Unity.

## Задание 1
### Найдите внутри C# скрипта “коэффициент корреляции ” и сделать выводы о том, как он влияет на обучение модели.
Я предположила, что в этой ситуации коэффициент корреляции будет равен максимальному расстоянию, на которое агент должен приблизиться к цели. В исходном скрипте это значение составляет 1,42.
Это значение позволяет соотнести ожидаемый результат агента с итогом его обучения.
При исходных значениях логирование выглядит следующим образом:
![image](https://github.com/user-attachments/assets/5e1c775c-a3d6-4c84-8fc7-436b66266e1f)

При изменении значени на 0,5:
![image](https://github.com/user-attachments/assets/ea502734-3caf-4174-a404-1c7e743ae6cf)
Процесс обучения занимает примерно одинаковое количество времени. Однако средняя награда значительно меньше, то есть агент реже достигает цели. Кроме того, увеличивается среднее отклонение от цели.
Более того, если проанализировать работу модели с таким значением, можно увидеть, что шарик начинает кружить рядом с кубом. Это увеличивает время поиска цели.

При изменении значении на 3:
![image](https://github.com/user-attachments/assets/92a92b00-0209-47c5-85fe-60f18aa5953f)
Время обучения немного увеличилось. Значение среднего вознаграждения как и при дефолтных значениях остаётся на уровне, близком к единице. Хотя шарик может и не касаться кубика, но будет засчитан как попадание, хотя визуально он находится далеко от цели.


## Задание 2
### Изменить параметры файла yaml-агента и определить какие параметры и как влияют на обучение модели. Привести описание не менее трех параметров.
Я изменила параметр batch_size с 10 на 50. Значение среднего вознаграждения практически не изменилось, оно осталось также близко к 1. А вот время обучения уменьшилось в 2 раза:
![image](https://github.com/user-attachments/assets/05c5475b-2cbf-4c15-9760-e3d231360c15)

Затем я изменила параметр num_epoch c 3 на 1. Время также оказалось меньше в 2 раза, относительно обучения на дефолтных знчениях. Средее вознаграждение осталось примерно таким же высоким:
![image](https://github.com/user-attachments/assets/648d09ab-4619-4a32-b8e6-fb7301d1d5d9)

Потом я поменяла параметр max_steps c 500000 на 200000. Время уменьшилось до 421 секунды. Среднее значение награды оказалось около 1. В итоге модель работает медленнее.
![image](https://github.com/user-attachments/assets/83fc213f-fa01-4d50-8eaf-c89c0b83d769)


## Задание 3
### Приведите примеры, для каких игровых задачи и ситуаций могут использоваться примеры 1 и 2 с ML-Agent’ом. В каких случаях проще использовать ML-агент, а не писать программную реализацию решения? 

- ML-агент по поиску цели:
  - Сбор предметов: Игрок или NPC может обучаться находить и собирать предметы, разбросанные по уровню. Например, в игре-выживании агент может научиться находить ресурсы, такие как еда или материалы.
  - Патрулирование: Агент может обучаться патрулированию определенной области, выявляя и реагируя на игрока или другие объекты. Это может быть полезно в стелс-играх.
  - Поиск укрытий: В шутерах агент может учиться находить укрытия при столкновении с противником, улучшая свою выживаемость.
  ML-агента проще использовать, когда необходимо создать сложное поведение, которое трудно формализовать в виде правил (например, адаптивное поведение противников).


- ML-агент-симулятор добычи ресурсов и их транспортировки:
  - Автоматизированный сбор ресурсов: Агент может обучаться добывать ресурсы (например, золото или древесину) и транспортировать их в базу, оптимизируя маршрут и минимизируя время.
  - Управление колонией: В стратегических играх агенты могут управлять добычей ресурсов для своей колонии, принимая решения о том, какие ресурсы собирать в зависимости от текущих потребностей.
  - Логистика: Агент может симулировать процесс транспортировки ресурсов между различными локациями, оптимизируя маршруты и распределение ресурсов.
  ML-агента проще использовать, когда необходимо учитывать множество переменных и неопределенности (например, динамически изменяющееся окружение или конкуренция за ресурсы).

## Выводы

В этой практической работе я узнала, что такое ML-агент и как с ним работать. Посмотрела, как обучаются модели и какие параметры на них влияют.

| Plugin | README |
| ------ | ------ |
| Dropbox | [plugins/dropbox/README.md][PlDb] |
| GitHub | [plugins/github/README.md][PlGh] |
| Google Drive | [plugins/googledrive/README.md][PlGd] |
| OneDrive | [plugins/onedrive/README.md][PlOd] |
| Medium | [plugins/medium/README.md][PlMe] |
| Google Analytics | [plugins/googleanalytics/README.md][PlGa] |

## Powered by

**BigDigital Team: Denisov | Fadeev | Panov**
